{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1oNiQkyGOip"
      },
      "source": [
        "## Question-2 Part2- English to Spanish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SBAsI-lGPRS"
      },
      "source": [
        "### Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf9iolIuwcJx"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "from __future__ import print_function, division\n",
        "import torch.optim as optim\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import pickle\n",
        "import os\n",
        "from torchvision import datasets, models, transforms\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import heapq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT5K3uQCw4s-",
        "outputId": "282e39ac-f488-4b53-f11f-741633c280d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/IDS576_Assignment_3\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/IDS576_Assignment_3\n",
        "cuda = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtZYf9UIxAeX"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "import string\n",
        "import torch.nn.functional as F\n",
        "cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SOS_tkn = 0\n",
        "EOS_tkn = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGVHdoR9xUfs"
      },
      "outputs": [],
      "source": [
        "class Lang:\n",
        "  def __init__(self, name):\n",
        "      self.name = name\n",
        "      self.wordToIndex = {}\n",
        "      self.word2count = {}\n",
        "      self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "      self.min_words = 2\n",
        "\n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self, word):\n",
        "    if word not in self.wordToIndex:\n",
        "      self.wordToIndex[word] = self.min_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.min_words] = word\n",
        "      self.min_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wjn3acoHxa57"
      },
      "outputs": [],
      "source": [
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.wordToIndex = {}\n",
        "        self.wordToCount = {}\n",
        "        self.indexToword = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.min_words = 2\n",
        "        self.words=[]\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.wordToIndex:\n",
        "            self.wordToIndex[word] = self.min_words\n",
        "            self.wordToCount[word] = 1\n",
        "            self.indexToword[self.min_words] = word\n",
        "            self.words.append(word)\n",
        "            self.min_words += 1\n",
        "        else:\n",
        "            self.wordToCount[word] += 1\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxqGR0EGxx3t"
      },
      "outputs": [],
      "source": [
        "def readLangs(lng_1, lng_2, reverse=False):\n",
        "    print(\"Reading data lines...\")\n",
        "\n",
        "    lines = open('/content/drive/MyDrive/IDS576_Assignment_3/spa-eng/spa.txt' , encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    l=[]\n",
        "    for s in pairs:\n",
        "      l.append(s[0:2])\n",
        "    pairs = l\n",
        "    pairs\n",
        "\n",
        "\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        inp_lang , op_lang= Lang(lng_2) ,Lang(lng_1)\n",
        "\n",
        "    else:\n",
        "        inp_lang, op_lang = Lang(lng_1) , Lang(lng_2)\n",
        "\n",
        "    return inp_lang, op_lang, pairs\n",
        "\n",
        "MX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\"i am \", \"i m \", \"he is\", \"he s \", \"she is\", \"she s \",\"you are\", \"you re \", \"we are\", \"we re \", \"they are\", \"they re \")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H28IE2XYx58p"
      },
      "outputs": [],
      "source": [
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS8Gu8yYx__9"
      },
      "outputs": [],
      "source": [
        "def prepareData(lng_1, lng_2, reverse=False):\n",
        "    inp_lang, op_lang, pairs = readLangs(lng_1, lng_2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting the words...\")\n",
        "    for pair in pairs:\n",
        "        inp_lang.addSentence(pair[0])\n",
        "        op_lang.addSentence(pair[1])\n",
        "    print(\"Words Counted:\")\n",
        "    print(inp_lang.name, inp_lang.min_words)\n",
        "    print(op_lang.name, op_lang.min_words)\n",
        "    return inp_lang, op_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu83nOXMyOuT",
        "outputId": "0d20bf19-50c1-4937-c5f6-7562d3fb238e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data lines...\n",
            "Read 118964 sentence pairs\n",
            "Trimmed to 90751 sentence pairs\n",
            "Counting the words...\n",
            "Words Counted:\n",
            "eng 10763\n",
            "spa 20221\n",
            "['that s not a question .', 'no es una pregunta .']\n"
          ]
        }
      ],
      "source": [
        "inp_lang, op_lang, pairs = prepareData('eng', 'spa')\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTypoalvGYXn"
      },
      "source": [
        "### Loading glove.6b.100d.txt File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_IpB2Gzveie"
      },
      "outputs": [],
      "source": [
        "emmbed_dict = {}\n",
        "with open('/content/drive/MyDrive/IDS576_Assignment_3/glove.6B.100d.txt','r') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:],'float32')\n",
        "    emmbed_dict[word]=vector\n",
        "\n",
        "\n",
        "def glove_embeding (glove_word):\n",
        "  try:\n",
        "    gloveVector = emmbed_dict[glove_word]\n",
        "  except KeyError:\n",
        "    gloveVector = np.random.normal(scale=0.6, size=(100, ))\n",
        "  return gloveVector\n",
        "\n",
        "matrix_len = inp_lang.min_words\n",
        "weightsMatrix = np.zeros((matrix_len, 100))\n",
        "words_found = 0\n",
        "\n",
        "for i, word in enumerate(pairs[0]):\n",
        "    try:\n",
        "        weightsMatrix[i] = emmbed_dict[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weightsMatrix[i] = np.random.normal(scale=0.6, size=(100, ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7yR2LOE9SMX"
      },
      "outputs": [],
      "source": [
        "def create_emb_layer(weightsMatrix, non_trainable=False):\n",
        "    numEmbeddings, embeddingDim = weightsMatrix.shape\n",
        "    emb_layer = nn.Embedding(numEmbeddings, embeddingDim)\n",
        "    emb_layer.load_state_dict({'weight':torch.tensor(weightsMatrix, dtype=torch.long, device=cuda) })\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "\n",
        "    return emb_layer, numEmbeddings, embeddingDim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YFld1yjybna"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hdnSize):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hdnSize = hdnSize\n",
        "        self.embedding, numEmbeddings, embeddingDim = create_emb_layer(weightsMatrix, True)\n",
        "        #self.embedding = nn.Embedding(input_size, hdnSize)\n",
        "        self.gru = nn.GRU(hdnSize, hdnSize)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hdnSize, device=cuda)\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, hdnSize, output_size):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.hdnSize = hdnSize\n",
        "    self.embedding = nn.Embedding(output_size, hdnSize)\n",
        "    self.gru = nn.GRU(hdnSize, hdnSize)\n",
        "    self.out = nn.Linear(hdnSize, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "def forward(self, input, hidden):\n",
        "    output = self.embedding(input).view(1, 1, -1)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.softmax(self.out(output[0]))\n",
        "    return output, hidden\n",
        "def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hdnSize, device=cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcUGZhmPyoCd"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "  def __init__(self, hdnSize, output_size, dropout_p=0.1, max_length=MX_LENGTH):\n",
        "    super(AttnDecoderRNN, self).__init__()\n",
        "    self.hdnSize = hdnSize\n",
        "    self.output_size = output_size\n",
        "    self.dropout_p = dropout_p\n",
        "    self.max_length = max_length\n",
        "\n",
        "    self.embedding = nn.Embedding(self.output_size, self.hdnSize)\n",
        "    self.attn = nn.Linear(self.hdnSize * 2, self.max_length)\n",
        "    self.attn_combine = nn.Linear(self.hdnSize * 2, self.hdnSize)\n",
        "    self.dropout = nn.Dropout(self.dropout_p)\n",
        "    self.gru = nn.GRU(self.hdnSize, self.hdnSize)\n",
        "    self.out = nn.Linear(self.hdnSize, self.output_size)\n",
        "\n",
        "  def forward(self, input, hidden, encoderOutputs):\n",
        "      embedded = self.embedding(input).view(1, 1, -1)\n",
        "      embedded = self.dropout(embedded)\n",
        "\n",
        "      attn_weights = F.softmax(\n",
        "        self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "      attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoderOutputs.unsqueeze(0))\n",
        "\n",
        "      output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "      output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "      output = F.relu(output)\n",
        "      output, hidden = self.gru(output, hidden)\n",
        "\n",
        "      output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "      return output, hidden, attn_weights\n",
        "\n",
        "  def initHidden(self):\n",
        "      return torch.zeros(1, 1, self.hdnSize, device=cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xr-pWs6y3kM"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "  return [lang.wordToIndex[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "  indexes = indexesFromSentence(lang, sentence)\n",
        "  indexes.append(EOS_tkn)\n",
        "  return torch.tensor(indexes, dtype=torch.long, device=cuda).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "  inpTensor = tensorFromSentence(inp_lang, pair[0])\n",
        "  target_tensor = tensorFromSentence(op_lang, pair[1])\n",
        "  return (inpTensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-Nhx-_Xy8UX"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtdTsj0qzFuB"
      },
      "outputs": [],
      "source": [
        "def train(inpTensor, target_tensor, encoder, decoder, encoderOptimizer, decoderOptimizer, criterion, max_length=MX_LENGTH):\n",
        "  encoderHdn = encoder.initHidden()\n",
        "\n",
        "  encoderOptimizer.zero_grad()\n",
        "  decoderOptimizer.zero_grad()\n",
        "  input_length = inpTensor.size(0)\n",
        "  target_length = target_tensor.size(0)\n",
        "  encoderOutputs = torch.zeros(max_length, encoder.hdnSize, device=cuda)\n",
        "  loss = 0\n",
        "\n",
        "  for ei in range(input_length):\n",
        "    encoder_output, encoderHdn = encoder(\n",
        "      inpTensor[ei], encoderHdn)\n",
        "    encoderOutputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "  decoder_input = torch.tensor([[SOS_tkn]], device=cuda)\n",
        "  decoder_hidden = encoderHdn\n",
        "  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "  if use_teacher_forcing:\n",
        "    # Teacher forcing: Feed the target as the next input\n",
        "    for di in range(target_length):\n",
        "      decoderOpt, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoderOutputs)\n",
        "      loss += criterion(decoderOpt, target_tensor[di])\n",
        "      decoder_input = target_tensor[di] # Teacher forcing\n",
        "  else:\n",
        "    # Without teacher forcing: use its own predictions as the next input\n",
        "    for di in range(target_length):\n",
        "      decoderOpt, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoderOutputs)\n",
        "      topv, topi = decoderOpt.topk(1)\n",
        "      decoder_input = topi.squeeze().detach() # detach from history as input\n",
        "\n",
        "      loss += criterion(decoderOpt, target_tensor[di])\n",
        "      if decoder_input.item() == EOS_tkn:\n",
        "        break\n",
        "  loss.backward()\n",
        "\n",
        "  encoderOptimizer.step()\n",
        "  decoderOptimizer.step()\n",
        "\n",
        "  return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laJv_pY2zVkX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcv3oKduzZIj"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plotLossTotal = 0  # Reset every plot_every\n",
        "\n",
        "    encoderOptimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoderOptimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        inpTensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(inpTensor, target_tensor, encoder,\n",
        "                     decoder, encoderOptimizer, decoderOptimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plotLossTotal += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plotLossTotal / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plotLossTotal = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-qRZZhkzk3I"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "  plt.figure()\n",
        "  fig, ax = plt.subplots()\n",
        "  # this locator puts ticks at regular intervals\n",
        "  loc = ticker.MultipleLocator(base=0.2)\n",
        "  ax.yaxis.set_major_locator(loc)\n",
        "  plt.plot(points)\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, max_length=MX_LENGTH):\n",
        "  with torch.no_grad():\n",
        "    inpTensor = tensorFromSentence(inp_lang, sentence)\n",
        "    input_length = inpTensor.size()[0]\n",
        "    encoderHdn = encoder.initHidden()\n",
        "\n",
        "    encoderOutputs = torch.zeros(max_length, encoder.hdnSize, device=cuda)\n",
        "\n",
        "    for ei in range(input_length):\n",
        "      encoder_output, encoderHdn = encoder(inpTensor[ei], encoderHdn)\n",
        "      encoderOutputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_tkn]], device=cuda) # SOS\n",
        "\n",
        "    decoder_hidden = encoderHdn\n",
        "\n",
        "    decoded_words = []\n",
        "    decoderAttentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "    for di in range(max_length):\n",
        "      decoderOpt, decoder_hidden, decoder_attention = decoder(\n",
        "        decoder_input, decoder_hidden, encoderOutputs)\n",
        "      decoderAttentions[di] = decoder_attention.data\n",
        "      topv, topi = decoderOpt.data.topk(1)\n",
        "      if topi.item() == EOS_tkn:\n",
        "        decoded_words.append('<EOS>')\n",
        "        break\n",
        "      else:\n",
        "        decoded_words.append(op_lang.indexToword[topi.item()])\n",
        "\n",
        "      decoder_input = topi.squeeze().detach()\n",
        "\n",
        "    return decoded_words, decoderAttentions[:di + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTk9wEkPztWU"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMn8EIw_OKLE",
        "outputId": "3b9459a6-67b2-4634-9116-f431f9dd8b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2m 52s (- 11m 30s) (10000 20%) 5.0839\n",
            "5m 44s (- 8m 37s) (20000 40%) 4.8676\n",
            "8m 39s (- 5m 46s) (30000 60%) 4.7886\n",
            "11m 33s (- 2m 53s) (40000 80%) 4.7361\n",
            "14m 28s (- 0m 0s) (50000 100%) 4.7086\n"
          ]
        }
      ],
      "source": [
        "hdnSize = 100\n",
        "encoder1 = EncoderRNN(inp_lang.min_words, hdnSize).to(cuda)\n",
        "attn_decoder1 = AttnDecoderRNN(hdnSize, op_lang.min_words, dropout_p=0.1).to(cuda)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 50000, print_every=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20jZ0J9WCDhd"
      },
      "source": [
        "### Examples of English to Spanish Translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6jhMi6mDNI-",
        "outputId": "ba4f5707-f550-4b78-bf59-b5fb7a9aad4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> he hates air travel .\n",
            "= odia los viajes en avion .\n",
            "<  es es . . <EOS>\n",
            "\n",
            "> can you tell me what this word means ?\n",
            "=  puede decirme que significa esta palabra ?\n",
            "<  no que de en . . . <EOS>\n",
            "\n",
            "> he s friendly with all his classmates .\n",
            "= el es amistoso con todos sus companeros .\n",
            "<  no que de . . . . <EOS>\n",
            "\n",
            "> she was crying in her room .\n",
            "= ella estaba llorando en su habitacion .\n",
            "<  no que de . . . <EOS>\n",
            "\n",
            "> she complained to him about everything .\n",
            "= se quejaba de todo con el .\n",
            "<  no que de . . . <EOS>\n",
            "\n",
            "> what does tom think about this ?\n",
            "=  que opina tom de esto ?\n",
            "<  no que de . . . <EOS>\n",
            "\n",
            "> you have a great job .\n",
            "= tienes un buen trabajo .\n",
            "<  es que . . . <EOS>\n",
            "\n",
            "> tom can run fast .\n",
            "= tom puede correr rapido .\n",
            "<  es es . . <EOS>\n",
            "\n",
            "> she spends a lot of money on shoes .\n",
            "= ella gasta mucho dinero en zapatos .\n",
            "<  no que de en el . . <EOS>\n",
            "\n",
            "> they never came .\n",
            "= ellos nunca vinieron .\n",
            "<  es . . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Ww1s4REyZf"
      },
      "source": [
        "#We now translate the sentences from Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2hYRlSGJwXM"
      },
      "outputs": [],
      "source": [
        "def translate(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHiVb5YnCSox",
        "outputId": "ad7120f6-78c2-49ae-e35f-c24fd4e92b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = she had a an hat\n",
            "output =  es es . . <EOS>\n"
          ]
        }
      ],
      "source": [
        "translate('she had a an hat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmpNki0fCW8f",
        "outputId": "9609c50e-1651-490a-800d-3ebd2bed1c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = how they re you\n",
            "output =  es . . <EOS>\n"
          ]
        }
      ],
      "source": [
        "translate('how they re you')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFf-GEsVEL2k",
        "outputId": "e268ad18-cd04-432d-9627-c318dce5a21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = look the the the\n",
            "output =  es . . <EOS>\n"
          ]
        }
      ],
      "source": [
        "translate('look the the the')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXAjG9ZnEL8-",
        "outputId": "17de70e4-8f7f-4afb-90f8-622e2ac9ee43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = i m good\n",
            "output =  es . <EOS>\n"
          ]
        }
      ],
      "source": [
        "translate('i m good')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfQBx44eEMB4",
        "outputId": "339a337b-956a-46e1-b879-3a87cb98b071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = i am a big person\n",
            "output =  es es . . <EOS>\n"
          ]
        }
      ],
      "source": [
        "translate('i am a big person')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIVY2dURNIHw"
      },
      "source": [
        "## Here, We observed in the final translation for the language conversion eng to esp more training is needed to be done with a lot more training words since in the output only a few words are getting detected"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}